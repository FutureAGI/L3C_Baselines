---
model_config:
  vocabulary_size: 256
  transformer_hidden_size: 1024
  transformer_nhead: 16
  n_transformer_block: 12
  max_time_step: 1024
  loss_context_warmup: 512
  vocab_size: 32
  load_model_path: ./checkpoints/01
  save_model_path: ./checkpoints/

train_config:
  batch_size: 1
  file_size: 500
  time_step: 1024
  learning_rate: 1.0e-3
  learning_rate_noam_decay_interval: 1000
  data_path: ./data
  load_model_path: ./checkpoints/01

test_config:
  batch_size: 1
  file_size: 500
  time_step: 1024
  data_path: ./data

