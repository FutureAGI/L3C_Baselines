---
model_config:
  transformer_hidden_size: 256
  transformer_nhead: 8
  n_transformer_block: 12
  max_time_step: 4096
  loss_context_warmup: 2048
  vocabulary_size: 32

train_config:
  batch_size: 4
  file_size: 500
  time_step: 4096
  max_epochs: 10
  use_amp: true
  use_scaler: false
  learning_rate: 1.0e-3
  learning_rate_noam_decay_interval: 5000
  start_step: 30000
  data_path: ./data
  load_model_path: ./checkpoints2/06
  save_model_path: ./checkpoints
  master_port: "12301"
  evaluation_interval: 1

test_config:
  batch_size: 4
  file_size: 500
  time_step: 1024
  data_path: ./data
  master_port: "12303"
  load_model_path: [PATH]

demo_config:
  vocab: english
  load_model_path: [PATH]
  master_port: "12301"
