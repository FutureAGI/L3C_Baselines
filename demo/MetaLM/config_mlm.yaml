---
model_config:
  transformer_hidden_size: 1024
  transformer_nhead: 16
  n_transformer_block: 12
  max_time_step: 4096
  loss_context_warmup: 512
  vocabulary_size: 32

train_config:
  batch_size: 1
  file_size: 500
  time_step: 1024
  max_epochs: 10
  learning_rate: 1.0e-3
  learning_rate_noam_decay_interval: 1000
  data_path: ./data
  load_model_path: null
  save_model_path: ./checkpoints/
  master_port: "12300"

test_config:
  batch_size: 1
  file_size: 500
  time_step: 1024
  data_path: ./data
  master_port: "12301"
  load_model_path: ./checkpoints/01

demo_config:
  vocab: english
  load_model_path: ./checkpoints/01
  master_port: "12301"
