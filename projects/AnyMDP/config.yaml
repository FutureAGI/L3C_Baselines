---
run_name: [RUN_NAME]
master_port: "12402"
load_model_path: [Path or None]

log_config:
  use_tensorboard: True
  tensorboard_log: [Path]
  training_log: [Path]
  evaluation_log: [Path]

model_config:
  context_warmup: 512
  max_position_loss_weighting: 48000
  action_dim: 5
  policy_loss_type: CrossEntropy
  rsa_type: sar  #psa/sar/psar/sa

  state_encode:
      input_type: Discrete
      input_size: 128
      hidden_size: 512
      dropout: 0.0

  action_encode:
      input_type: Discrete
      input_size: 6
      hidden_size: 512
      dropout: 0.0

  prompt_encode:
      input_type: Continuous
      input_size: 1
      hidden_size: 512
      dropout: 0.0

  reward_encode:
      input_type: Continuous
      input_size: 1
      hidden_size: 512
      dropout: 0.0

  state_decode:
      output_type: Discrete
      input_size: 512
      hidden_size:
          - 512
          - 128
      layer_norm: True
      residual_connect: True
      dropout: 0.0

  action_decode:
      output_type: Discrete
      input_size: 512
      hidden_size:
          - 512
          - 6
      layer_norm: True
      residual_connect: True
      dropout: 0.0

  reward_decode:
      output_type: Continuous
      input_size: 512
      hidden_size:
          - 512
          - 1
      layer_norm: True
      residual_connect: True
      dropout: 0.0

  causal_block_bak_1: # change to causal_block if use this block
      model_type: TRANSFORMER
      num_layers: 6
      hidden_size: 512
      nhead: 8
      inner_hidden_size: 512
      dropout: 0.10
      context_window: -1
      checkpoints_density: -1
      position_encoding_size: 12000
      use_layer_norm: True
      use_blockrecurrence: True
      memory_length: 3000
      memory_type: KV

  causal_block_bak_2:
      model_type: GSA # GLA
      num_layers: 12
      hidden_size: 512
      inner_hidden_size: 1024
      dropout: 0.10
      nhead: 4
      memory_length: 64
      position_encoding_size: 12000
      use_layer_norm: True
      use_blockrecurrence: True
      checkpoints_density: -1
      memory_type: MEM

  causal_block_bak_3:
      model_type: RWKV6
      num_layers: 12
      hidden_size: 512
      inner_hidden_size: 1024
      dropout: 0.10
      nhead: 16
      expand_k: 1
      expand_v: 2
      hidden_ratio: 3.5
      position_encoding_size: 12000
      use_layer_norm: True
      use_blockrecurrence: True
      checkpoints_density: -1
      memory_length: 0
      memory_type: MEM

  causal_block_bak_4:
      model_type: Mamba
      num_layers: 12
      hidden_size: 512
      inner_hidden_size: 1024
      expand: 2
      d_conv: 4
      d_state: 16
      dropout: 0.10
      position_encoding_size: 12000
      use_layer_norm: True
      use_blockrecurrence: True
      checkpoints_density: -1
      memory_length: 0
      memory_type: MEM

train_config:
    max_epochs: 20
    batch_size: 12

    seq_len: 16000
    seg_len: 4000

    lr: 2.0e-4
    lr_decay_interval: 2000
    lr_start_step: 8000

    data_path: [Path]
    save_model_path: [Path]
    max_save_iterations: 1000

    lossweight_policymodel: 0.2
    lossweight_worldmodel_states: 0.4
    lossweight_worldmodel_rewards: 0.4
    lossweight_entropy: 0.0
    lossweight_l2: 0.0

    use_amp: False
    use_scaler: False

test_config:
    batch_size: 16
    data_path: [Path]
    output: "./offline_eval/"
    seq_len: 16000
    seg_len: 4000

generator_config:
    action_clip: 4
    decoding_strategy:
        T_ini: 1.0
        T_fin: 0.1
        decay_type: Linear  # Exponential
    max_steps: 16000
    output: "./online_eval/"